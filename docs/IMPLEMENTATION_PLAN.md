# Fine-tuning κµ¬ν„ κ³„νμ„

**ν”„λ΅μ νΈ**: Metric Learning κΈ°λ° μ¤νƒ€μΌ λ§¤μΉ­ μ‹μ¤ν…  
**μ‘μ„±μΌ**: 2025-12-05  
**ν„μ¬ μƒνƒ**: Epoch 10 ν•™μµ μ™„λ£

---

## 1. ν„μ¬ λ¨λΈ μ„±λ¥

| Epoch | Train Loss | Avg Triplets | Val R@1 | MAP@R  |
| :---: | :--------: | :----------: | :-----: | :----: |
|   1   |   0.1467   |    145.2     | 79.59%  | 0.7171 |
|  10   |   0.1324   |     47.1     | 86.73%  | 0.7879 |

**λ¶„μ„**: Avg Tripletsκ°€ 145 β†’ 47λ΅ κ°μ†ν•μ—¬ ν•™μµμ΄ μ§„ν–‰ μ¤‘μ΄λ‚, κ³ μ›(Plateau) μƒνƒ μ§„μ… κ°€λ¥μ„± μμ.

---

## 2. Fine-tuning λ©ν‘

| μ§€ν‘         | ν„μ¬ (Epoch 10) | λ©ν‘ (Epoch 15) |
| ------------ | --------------- | --------------- |
| Val R@1      | 86.73%          | **88%+**        |
| MAP@R        | 0.7879          | **0.80+**       |
| Avg Triplets | 47.1            | **35 μ΄ν•**     |

---

## 3. νλΌλ―Έν„° λ³€κ²½ μ‚¬ν•­

### 3.1 μμ •λ Config (`scripts/train.py`)

```python
# λ³€κ²½ μ „ β†’ λ³€κ²½ ν›„
epochs: int = 10 β†’ 15          # +5 Epoch μ¶”κ°€
learning_rate: float = 1e-4 β†’ 5e-5   # LR μ λ°μΌλ΅ κ°μ†
output_dir: str = "./checkpoints" β†’ "./checkpoints_finetuned"
resume_from_checkpoint: None β†’ "./checkpoints/checkpoint_epoch_10.pth"
```

### 3.2 μ μ§€λλ” μ„¤μ •

| νλΌλ―Έν„°      | κ°’       | λΉ„κ³                 |
| ------------- | -------- | ------------------- |
| margin        | 0.3      | μ μ§€                |
| miner_type    | semihard | μ μ§€                |
| P, K          | 5, 4     | μ μ§€ (λ°°μΉ ν¬κΈ° 20) |
| freeze_vision | True     | λ°±λ³Έ λ™κ²° μ μ§€      |

---

## 4. μ‹¤ν–‰ λ°©λ²•

```bash
# EC2 μΈμ¤ν„΄μ¤μ—μ„ μ‹¤ν–‰
cd /path/to/NLP-Character-feature-extraction
python scripts/train.py
```

**μμƒ μ†μ” μ‹κ°„**: μ•½ 25μ‹κ°„ (5 Epoch Γ— 5μ‹κ°„/Epoch)

---

## 5. λ¨λ‹ν„°λ§ κΈ°μ¤€

### 5.1 μ •μƒ μ§„ν–‰ μ‹ νΈ

- Val R@1: 87% μ΄μƒ μ μ§€
- Avg Triplets: κ°μ† μ¶”μ„Έ
- Val Loss: 0.11 μ΄ν•

### 5.2 μ¤‘λ‹¨ κΈ°μ¤€ π¨

| μƒν™©      | μ΅°κ±΄                       | μ΅°μΉ              |
| --------- | -------------------------- | ----------------- |
| κ³Όμ ν•©    | Val Loss > 0.12            | μ¦‰μ‹ μ¤‘λ‹¨         |
| μ„±λ¥ μ €ν• | R@1 2ν μ—°μ† ν•λ½          | ν•™μµ μ¤‘λ‹¨ ν›„ λ΅¤λ°± |
| ν•™μµ μ •μ²΄ | Avg Triplets λ³€ν™” μ—†μ 3ν | LR μ¶”κ°€ κ°μ† κ²€ν†  |

---

## 6. ν›„μ† μ΅°μΉ

### Case A: λ©ν‘ λ‹¬μ„± μ‹ (R@1 β‰¥ 88%)

1. `checkpoints_finetuned/best_model.pth` μµμΆ… λ¨λΈλ΅ μ±„νƒ
2. `research.md` κ²°κ³Όκ°’ μ—…λ°μ΄νΈ
3. μ¶”κ°€ 5 Epoch ν•™μµ κ²€ν† 

### Case B: μ„±λ¥ μ •μ²΄ μ‹ (R@1 < 87%)

1. LRμ„ 2e-5λ΅ λ” λ‚®μ¶°μ„ μ¬μ‹λ„
2. λλ” Marginμ„ 0.35λ΅ μ¦κ°€ μ‹λ„

### Case C: κ³Όμ ν•© λ°μƒ μ‹

1. κ°€μ¥ μΆ‹μ€ μ²΄ν¬ν¬μΈνΈλ΅ λ΅¤λ°±
2. Dropout μ¶”κ°€ λλ” λ°μ΄ν„° μ¦κ°• μ μ©

---

## 7. μ²΄ν¬λ¦¬μ¤νΈ

- [x] `train.py` Config μμ • μ™„λ£
- [x] `research.md` λ…Όλ¬Έ λ‚΄μ© μμ • (Semi-hard Mining)
- [ ] Fine-tuning μ‹¤ν–‰ (Epoch 11~15)
- [ ] κ²°κ³Ό λ¶„μ„ λ° `research.md` μ—…λ°μ΄νΈ
- [ ] μµμΆ… λ¨λΈ μ„ μ •
